{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from path_fns.filepaths import TRANSFORMED_MASTER_DATA_ONE_ROW_PER_PERSON\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persons = pd.read_parquet(TRANSFORMED_MASTER_DATA_ONE_ROW_PER_PERSON)\n",
    "len(df_persons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persons.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persons.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in [\"humanLabel\", \"humanAltLabel\", \"birth_name\", \"given_nameLabel\", \"family_nameLabel\"]:\n",
    "    df_persons[col] = df_persons[col].map(lambda x: list(map(str.lower, x)))\n",
    "\n",
    "df_persons.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persons[\"occupationLabel\"].head(1000).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_data.names import get_df_given_names_with_freqs, get_df_family_names_with_freqs\n",
    "df_given_names = get_df_given_names_with_freqs(spark)\n",
    "df_family_names = get_df_family_names_with_freqs(spark)\n",
    "df_given_names.createOrReplaceTempView(\"df_given_names\")\n",
    "df_family_names.createOrReplaceTempView(\"df_family_names\")\n",
    "display(df_given_names.limit(2).toPandas())\n",
    "display(df_family_names.limit(2).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person postcode lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_data.utils import get_person_nearby_postcodes_lookup\n",
    "df_point_postcode = get_person_nearby_postcodes_lookup(spark)\n",
    "df_point_postcode.createOrReplaceTempView(\"df_point_postcode\")\n",
    "\n",
    "\n",
    "df_point_postcode.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = spark.read.parquet(\"scrape_wikidata/raw_data/occupations/\")\n",
    "occupations.createOrReplaceTempView(\"occupations\")\n",
    "occupations.limit(2).toPandas()\n",
    "\n",
    "sql = \"\"\"\n",
    "select human, array_distinct(collect_list(occupationLabel)) as occupation_options\n",
    "from occupations\n",
    "group by human\n",
    "\"\"\"\n",
    "\n",
    "occupations = spark.sql(sql)\n",
    "occupations.createOrReplaceTempView(\"occupations\")\n",
    "occupations.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of given names and family names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_data.names import get_df_filter, name_split\n",
    "df_person = name_split(df_person, spark)\n",
    "df_person.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dupe_country_citizen = \"\"\"\n",
    "array_distinct(\n",
    "    transform(\n",
    "        split(h.country_citizenlabel, ' \\\\\\\\| '), \n",
    "            x -> case \n",
    "                    when x = 'United Kingdom of Great Britain and Ireland' then 'United Kingdom' \n",
    "                    else x \n",
    "                    end\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sql = f\"\"\"\n",
    "select \n",
    "h.row_num,\n",
    "h.human, \n",
    "h.humanlabel,\n",
    "split(h.humanaltlabel, \", \") as humanaltlabel,\n",
    "occ.occupation_options,\n",
    "substr(h.dob,1,10) as dob, \n",
    "\n",
    "\n",
    "{remove_dupe_country_citizen}  as country_citizenship,\n",
    "\n",
    "\n",
    "place_birthlabel as birth_place,\n",
    "birth_countrylabel as birth_country,\n",
    "sex_or_genderlabel as gender,\n",
    "\n",
    "\n",
    "\n",
    "residencelabel as residence_place,\n",
    "residence_countrylabel as residence_country,\n",
    "\n",
    "pc.birthplace_nearby_locs,\n",
    "pc.residence_nearby_locs,\n",
    "pc.random_nearby_locs,\n",
    "\n",
    "h.given_name_1 as given_name_1,\n",
    "n1.alt_names as alt_given_name_1,\n",
    "h.given_name_2 as given_name_2,\n",
    "n2.alt_names as alt_given_name_2,\n",
    "h.given_name_3 as given_name_3,\n",
    "n3.alt_names as alt_given_name_3,\n",
    "h.family_name_1 as family_name_1,\n",
    "n4.alt_names as alt_family_name_1,\n",
    "h.family_name_2 as family_name_2,\n",
    "n5.alt_names as alt_family_name_2\n",
    "\n",
    "from df_person as h\n",
    "\n",
    "left join df_given_names as n1\n",
    "on lower(h.given_name_1) = n1.original_name\n",
    "\n",
    "left join df_given_names as n2\n",
    "on lower(h.given_name_2) = n2.original_name\n",
    "\n",
    "left join df_given_names as n3\n",
    "on lower(h.given_name_3) = n3.original_name\n",
    "\n",
    "left join df_family_names as n4\n",
    "on lower(h.family_name_1) = n4.original_name\n",
    "\n",
    "left join df_family_names as n5\n",
    "on lower(h.family_name_2) = n5.original_name\n",
    "\n",
    "left join df_point_postcode as pc\n",
    "on h.human = pc.person\n",
    "\n",
    "left join occupations  as occ\n",
    "on h.human = occ.human\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "df_final = spark.sql(sql)\n",
    "df_final.createOrReplaceTempView(\"df_final\")  \n",
    "\n",
    "\n",
    "df_final.limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now want to link parent back onto this table, and their location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_family = spark.read.parquet(\"scrape_wikidata/raw_data/family/parent_child/\")\n",
    " \n",
    "df_one_parent = df_family.dropDuplicates([\"child\"])\n",
    "# df_one_child = df_family.dropDuplicates([\"human\"])\n",
    "df_one_parent.createOrReplaceTempView(\"df_one_parent\") \n",
    "# df_one_child.createOrReplaceTempView(\"df_one_child\") \n",
    "df_family.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "\n",
    "select \n",
    "    df.*, \n",
    "    p.human as parent, \n",
    "    p_details.birthplace_nearby_locs as parent_birthplace_loc, \n",
    "    p_details.random_nearby_locs as parent_random_loc\n",
    "\n",
    "\n",
    "from df_final as df\n",
    "left join df_one_parent as p\n",
    "on df.human = p.child\n",
    "left join df_final as p_details\n",
    "on p.human = p_details.human\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# No need for child as if parents take childs details and children take parents, then you don't achieve parents and children having the same address!\n",
    "# left join df_one_child as c\n",
    "# on df.human = c.human\n",
    "# left join df_final as c_details\n",
    "# on c.child = c_details.human\n",
    "df_final_with_parents = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_with_parents = df_final_with_parents.repartition(100)\n",
    "df_final_with_parents.write.mode('overwrite').parquet(\"scrape_wikidata/clean_data/master_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os \n",
    "files = sorted(\n",
    "    glob.glob(\"scrape_wikidata/clean_data/master_data/*.parquet\")\n",
    ")\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    \n",
    "    base = os.path.basename(f)\n",
    "    f2 = f.replace(base, f\"{i:03}.parquet\")\n",
    "    \n",
    "    os.rename(f, f2)\n",
    "    \n",
    "files = sorted(\n",
    "    glob.glob(\"scrape_wikidata/clean_data/master_data/.*crc\")\n",
    ")\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"corrupted_data/uk_citizens_groupsize_20/\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('splink-synthetic-data-wgdmJF1G-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8eb8e75dda806d3ac6e26aaf7be42b5929aa5dc2c51ff4b4461a839a1090eb14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
